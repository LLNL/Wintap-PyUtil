{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started using parquet via DuckDB with Wintap\n",
    "\n",
    "## Workflow:\n",
    "Parquet files -> DuckDB Tables/Views -> SQL EDA/Extraction -> Pandas -> Resume typical workflow\n",
    "\n",
    "The motivation for introducing DuckDB to the workflow for initial EDA and extraction of subsets is to allow for working with datasets larger than memory. Once the subset of interest is identified using SQL, the result can be extracted easily into pandas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map parquet into DuckDB\n",
    "* Initialize an in-memory database with views for all event types at an aggregation level.\n",
    "    * Note that views are basically pointers to the parquet files and use no memory.\n",
    "* Present a summary of current dataset\n",
    "    * Tabular view with row counts and parquet file sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define imports, functions\n",
    "# This dataset_chooser() uses a .env file in the top level of this project. It needs to define DATAPATH as the top level of where your data sets are.\n",
    "# You can optionally define a DEFAULT_PATH pointing to a specific dataset. This provides the convenience of not having to select the dataset when restarting the notebook.\n",
    "# See .env-default for an example.\n",
    "# If there is no .env or the paths are invalid, dataset_chooser() defaults to users home directory.\n",
    "\n",
    "# To enable logging output to jupyter, uncomment the following 3 lines:\n",
    "#import logging\n",
    "#logger = logging.getLogger()\n",
    "#logger.setLevel(logging.DEBUG)\n",
    "%run notebookutil.py\n",
    "\n",
    "w_datasets=dataset_chooser()\n",
    "display(w_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an in-memory db. Save reference in a variable and then set magic-duckdb environment. Result is ability to use the same DB instance from python code and %dql/%%dql magics.\n",
    "# Also create views for every top-level type found in the current dataset.\n",
    "con=ru.init_db(w_datasets.selected) # ,agg_level='rolling')\n",
    "%dql -co con\n",
    "# Display the list of tables/views\n",
    "%dql show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sets may have annotations in the form of discrete values interesting or sample data within them.\n",
    "# Load any that exist for the current dataset.\n",
    "# To Do: move this to notebookutil.py once its stabile.\n",
    "if os.path.exists(w_datasets.selected+'/annotations.py'):\n",
    "    %run $w_datasets.selected/annotations.py\n",
    "    %whos\n",
    "    display(SIMPLE)\n",
    "else:\n",
    "    print('No annotations defined for this dataset.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize event data and display in chart to help understand event distribution over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular summary\n",
    "display(svd.table_summary(con,w_datasets.selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events over time. \n",
    "# To do: Dynamically adjust the bucket size based on the dataset duration for the best resolution/performance.\n",
    "svd.init_db(con,SUMMARY_INTERVAL)\n",
    "eventdf=svd.fetch_summary_data(con)\n",
    "svd.display_event_chart(eventdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "* Summarize: display table schema and some statistics about its contents\n",
    "* Head: list a small set of rows\n",
    "* Group By: aggregate on 1-N columns\n",
    "* Time partitions: Filter or Group By Days using DayPK\n",
    "* Joining tables\n",
    "    * Within a single day: All systems go...\n",
    "    * Over multiple days: PROCESS and HOST both need to be deduped\n",
    "* Specific events: highlight events of interest (puttyx/notepad++/etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize process to get a high level view of the columns and values\n",
    "# Create a file with sample values per dataset.\n",
    "%dql -j summarize SELECT * FROM process where daypk BETWEEN {{MIN_DAYPK}} AND {{MAX_DAYPK}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all columns for the first 10 rows\n",
    "%dql select * from process limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all executions of a specific process by name\n",
    "%dql select pid_hash, first(process_name), first(daypk) daypk, count(*) from process where process_name = 'putty.exe' group by pid_hash order by daypk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dql -j\n",
    "-- Use GROUP BY to find the most and least common process_name. Jupyter helps out by displaying the first and last sets of rows.\n",
    "-- Calculate a counts for some common fields also.\n",
    "-- To keep it fast for demos, limit to a subset of DayPKs. Try commenting out the WHERE clause to see results over all the data.\n",
    "-- Note: the cell magic (%%dql) treats the entire cell as SQL, so python (#) comments do not work \n",
    "SELECT process_name, count(distinct hostname) num_hostname, count(distinct file_md5) num_file_md5, count(distinct user_name) num_user_name, count(distinct pid_hash), count(*) num_rows\n",
    "FROM process\n",
    "WHERE daypk BETWEEN {{MIN_DAYPK}} AND {{MAX_DAYPK}}\n",
    "GROUP BY ALL\n",
    "ORDER BY num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple count of processes per day, with result assigned to a panda\n",
    "# Convert dayPK to a timestamp and altair then displays it nicely.\n",
    "processes_per_day = %dql select strptime(dayPK,'%Y%m%d') dayPK, count(*) num_rows from process group by all order by daypk\n",
    "# Chart that using Altair\n",
    "chart = alt.Chart(processes_per_day).mark_line().encode(\n",
    "        x='dayPK:T',\n",
    "        y='num_rows',\n",
    "        tooltip=['dayPK:T','num_rows']\n",
    "    ).properties(\n",
    "        width=1200,\n",
    "        height=400,\n",
    "        title='Processes Per Day'\n",
    "    ).interactive()\n",
    "display(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single process and its network connections\n",
    "# Adding the daypk filter reduces the search space to just the single day rather than ~180 that are in the set.\n",
    "proc = %dql -j select * from process where pid_hash='{{SIMPLE.PID_HASH}}' and daypk={{SIMPLE.DAYPK}}\n",
    "net = %dql -j select * from process_net_conn where pid_hash='{{SIMPLE.PID_HASH}}' and daypk={{SIMPLE.DAYPK}} order by first_seen\n",
    "display(proc)\n",
    "display(net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign query result to a panda\n",
    "# This demonstrates using the single-line magic, so we'll keep the SQL short to be readable. Get all process_names for 1 day that used the network.\n",
    "%dql -j -o net_sum_df select p.process_name, count(distinct pnc.conn_id) num_conn_ids, count(*) num_rows from process p join process_net_conn pnc on pnc.pid_hash=p.pid_hash where p.dayPK={{SIMPLE.DAYPK}} and pnc.dayPK={{SIMPLE.DAYPK}} group by all order by all\n",
    "net_sum_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dql -j -o net_sum2_df\n",
    "-- Assign query result to a panda when using cell magic. \n",
    "-- With multiline, SQL can be formatted be more readable. Get all process_names for 1 day that used the network with additional features.\n",
    "select \n",
    "  p.process_name,\n",
    "  count(distinct p.hostname) num_hosts,\n",
    "  count(distinct p.user_name) num_users,\n",
    "  count(distinct pnc.conn_id) num_conn_ids,\n",
    "  count(distinct pnc.remote_port) num_remote_ports,\n",
    "  sum(tcp_recv_size) tcp_recv_size,\n",
    "  sum(tcp_send_size) tcp_send_size,\n",
    "  sum(udp_recv_size) udp_recv_size,\n",
    "  sum(udp_send_size) udp_send_size,\n",
    "  count(*) num_rows \n",
    "from process p \n",
    "join process_net_conn pnc on pnc.pid_hash=p.pid_hash \n",
    "-- Note: filtering both tables by dayPK dramatically increases speed at the cost of reducing the data scope.\n",
    "where p.dayPK={{SIMPLE.DAYPK}} and pnc.dayPK={{SIMPLE.DAYPK}}\n",
    "group by all \n",
    "order by all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file-based database with views to the current parquet data. Useful for opening directly as a DuckDB database from other tools.\n",
    "rollingdb=ru.init_db(w_datasets.selected,database='rolling.db')\n",
    "rollingdb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SQL that will map all event types into views. Does not execute the SQL.\n",
    "# Intended for generating SQL that will be executed in another context, such as the CLI or DBeaver.\n",
    "globs=ru.get_glob_paths_for_dataset(w_datasets.selected,'rolling')\n",
    "stmts=ru.generate_view_sql(globs)\n",
    "for sql in stmts:\n",
    "    print(sql.strip()+';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wintap-pyutil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
